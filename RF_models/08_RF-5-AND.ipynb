{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF including further locations with CrossValidation using SPA dataset\n",
    "\n",
    "The following locations are to be considered:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| longitude | latitude | country | nearest neighbour accession |\n",
    "| --- | --- | --- | :---: |\n",
    "| 36.76539 | -5.499419 | Andalucia | 1600 |\n",
    "| 51.49702 | 11.970655 | Germany | 1059 |\n",
    "| 65.00307 | 25.472679 | Finland | 309 |\n",
    "| 39.48083 | -0.340985 | Spain| 1576|\n",
    "| 52.62779 | 1.293458 | UK | 578 |\n",
    "| 48.544886 | 9.043042 | Tuebingen | 1813 |\n",
    "| 40.408049 | -3.83535 | Madrid | 1845 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "pd.set_option('display.max_columns', 999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select specific SNPs\n",
    "Select 1000 SNPs with highest and 1000 with lowest selection coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = pd.read_csv('/Carnegie/DPB/Data/Shared/Labs/Moi/Everyone/deepselection/randomForest/betas_woNAs_55climvars_rFit.txt', sep='\\t')\n",
    "betas.rename(columns={'clim-bio18.assoc_y':'clim-bio18'}, inplace=True)\n",
    "betas.drop(['clim-bio18.assoc_x'],axis=1, inplace=True)\n",
    "betas = betas[betas.columns.drop(list(betas.filter(regex='rFitness')))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas2 = pd.read_csv('/Carnegie/DPB/Data/Shared/Labs/Moi/Everyone/deepselection/randomForest/betas_woNAs_Fitness.txt', sep='\\t')\n",
    "betas2.rename(columns={'Fitness_Andaluci':'Fitness_Andalucia'}, inplace=True)\n",
    "betas2 = betas2[betas2.columns.drop(list(betas2.filter(regex='randomized')))]\n",
    "betas2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AND = betas2[['rs', 'Fitness_Andalucia']]\n",
    "SPA = betas2[['rs', 'Fitness_Spain']]\n",
    "UKI = betas2[['rs', 'Fitness_UnitedKingdom']]\n",
    "FIN = betas2[['rs', 'Fitness_Finland']]\n",
    "GER = betas2[['rs', 'Fitness_Germany']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort & select\n",
    "\n",
    "AND = AND.sort_values(by=['Fitness_Andalucia'], ascending=False)\n",
    "SPA = SPA.sort_values(by=['Fitness_Spain'], ascending=False)\n",
    "UKI = UKI.sort_values(by=['Fitness_UnitedKingdom'], ascending=False)\n",
    "FIN = FIN.sort_values(by=['Fitness_Finland'], ascending=False)\n",
    "GER = GER.sort_values(by=['Fitness_Germany'], ascending=False)\n",
    "\n",
    "x=1000\n",
    "\n",
    "selAND = AND.iloc[:x, :]   \n",
    "selAND = selAND.append(AND.iloc[-x:, :])\n",
    "selANDSNPs = selAND['rs'].tolist()\n",
    "\n",
    "selSPA = SPA.iloc[:x, :]   \n",
    "selSPA = selSPA.append(SPA.iloc[-x:, :])\n",
    "selSPASNPs = selSPA['rs'].tolist()\n",
    "\n",
    "selUKI = UKI.iloc[:x, :]   \n",
    "selUKI = selUKI.append(UKI.iloc[-x:, :])\n",
    "selUKISNPs = selUKI['rs'].tolist()\n",
    "\n",
    "selFIN = FIN.iloc[:x, :]   \n",
    "selFIN = selFIN.append(FIN.iloc[-x:, :])\n",
    "selFINSNPs = selFIN['rs'].tolist()\n",
    "\n",
    "selGER = GER.iloc[:x, :]   \n",
    "selGER = selGER.append(GER.iloc[-x:, :])\n",
    "selGERSNPs = selGER['rs'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use all rs from the selection above and create new list\n",
    "mySNPs = selANDSNPs + selSPASNPs + selUKISNPs + selFINSNPs + selGERSNPs\n",
    "len(mySNPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "from collections import Counter\n",
    "[k for k,v in Counter(mySNPs).items() if v>1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "mySNPs = list(set(mySNPs))\n",
    "\n",
    "# check again for duplicates\n",
    "from collections import Counter\n",
    "[k for k,v in Counter(mySNPs).items() if v>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mySNPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mySNPs)*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create now target dataframe with selected SNPs\n",
    "\n",
    "target = pd.DataFrame(mySNPs, columns=['rs'])\n",
    "\n",
    "e = target.join(AND.set_index('rs'), on='rs')\n",
    "e.rename(columns={'Fitness_Andalucia':'rFitness'}, inplace=True)\n",
    "e['locat'] = 'AND'\n",
    "\n",
    "f = target.join(GER.set_index('rs'), on='rs')\n",
    "f.rename(columns={'Fitness_Germany':'rFitness'}, inplace=True)\n",
    "f['locat'] = 'GER'\n",
    "\n",
    "g = target.join(FIN.set_index('rs'), on='rs')\n",
    "g.rename(columns={'Fitness_Finland':'rFitness'}, inplace=True)\n",
    "g['locat'] = 'FIN'\n",
    "\n",
    "h = target.join(SPA.set_index('rs'), on='rs')\n",
    "h.rename(columns={'Fitness_Spain':'rFitness'}, inplace=True)\n",
    "h['locat'] = 'SPA'\n",
    "\n",
    "i = target.join(UKI.set_index('rs'), on='rs')\n",
    "i.rename(columns={'Fitness_UnitedKingdom':'rFitness'}, inplace=True)\n",
    "i['locat'] = 'UKI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = e.append([f, g, h, i], ignore_index=True, sort=False)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count total number of NaNs\n",
    "target.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract locations where rFitness is NaN\n",
    "nullDF = target[target['rFitness'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nullDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nullSNPs = nullDF['rs'].tolist()\n",
    "#nullSNPs\n",
    "\n",
    "# check for duplicates\n",
    "from collections import Counter\n",
    "[k for k,v in Counter(mySNPs).items() if v>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nullSNPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop those rows of nullSNPs\n",
    "newtarget = target[~target.rs.isin(nullSNPs)] \n",
    "newtarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtarget.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = pd.DataFrame(mySNPs, columns=['rs'])\n",
    "predictors = predictors.join(betas.set_index('rs'), on='rs')\n",
    "predictors = pd.concat([predictors]*5, ignore_index=True)\n",
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add annotation to predictors dataset\n",
    "annot = pd.read_csv(f'/Carnegie/DPB/Data/Shared/Labs/Moi/Everyone/deepselection/randomForest/515g2.ann.txt', sep='\\t')\n",
    "predictors = predictors.join(annot.set_index('rs'), on='rs')\n",
    "predictors = predictors.drop(columns=['chr', 'ps', 'allel1', 'allel2'])\n",
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode annotation numerically\n",
    "lb = LabelEncoder()\n",
    "predictors['ann'] = lb.fit_transform(predictors['ann'])\n",
    "\n",
    "# print encoding\n",
    "lbMapping = dict(zip(lb.classes_, lb.transform(lb.classes_)))\n",
    "lbMapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare climate data \n",
    "\n",
    "clim = pd.read_csv(f'/Carnegie/DPB/Data/Shared/Labs/Moi/Everyone/natvar/climate/2029gclimate.tsv', delim_whitespace=True)\n",
    "\n",
    "climA = pd.concat([clim.iloc[[1600]]]*int(len(predictors)/5), ignore_index=True) #1600 = accession close to location Andalusia\n",
    "climG = pd.concat([clim.iloc[[1059]]]*int(len(predictors)/5), ignore_index=True) #1059 = accession close to location Germany\n",
    "climF = pd.concat([clim.iloc[[309]]]*int(len(predictors)/5), ignore_index=True) #309 = accession close to location Finland\n",
    "climS = pd.concat([clim.iloc[[1576]]]*int(len(predictors)/5), ignore_index=True) #1576 = accession close to location Spain\n",
    "climU = pd.concat([clim.iloc[[578]]]*int(len(predictors)/5), ignore_index=True) #578 = accession close to location United Kingdom\n",
    "\n",
    "climFin = pd.concat([climA, climG, climF, climS, climU], axis=0) #concat this way, to have Madrid at first, then TÃ¼bingen and then in alphabetic order to fit to target order\n",
    "climFin = climFin.iloc[:, :-12]    \n",
    "climFin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finalize predictors dataset\n",
    "predictors = pd.concat([predictors.reset_index(drop=True), climFin.reset_index(drop=True)], axis=1, sort=False)  # without reset_index, NAs were introduced in DF\n",
    "cols=[1,2,3,4]    #drop rFitness columns\n",
    "predictors = predictors.drop(predictors.columns[cols], axis=1)\n",
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with NaNs\n",
    "predictors = predictors[~predictors.rs.isin(nullSNPs)]  \n",
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entire = pd.concat([predictors.reset_index(drop=True), newtarget.reset_index(drop=True)], axis=1, sort=False)\n",
    "entire\n",
    "\n",
    "#drop one location --> for Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entire = entire.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for Cross Validation\n",
    "AND = entire[entire[\"locat\"] == 'AND']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for RF\n",
    "rest = entire[entire[\"locat\"] != 'AND']\n",
    "rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "### Input variable preparation and distribution plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = rest['rFitness']\n",
    "#X = predictors.iloc[:, 1:].copy()    # without rs column\n",
    "X_train = rest.iloc[:, 1:-3]\n",
    "y_test= AND['rFitness']\n",
    "X_test = AND.iloc[:, 1:-3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import QuantileTransformer, quantile_transform\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from yellowbrick.regressor import PredictionError, ResidualsPlot\n",
    "from yellowbrick.features import Rank1D\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit regression model\n",
    "regr_rf = RandomForestRegressor(oob_score=True, random_state=0, n_estimators = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "regr_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction of test set\n",
    "predicted_test = regr_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted_train = regr_rf.predict(X_train)\n",
    "test_score = r2_score(y_test, predicted_test)\n",
    "spearman = spearmanr(y_test, predicted_test)\n",
    "pearson = pearsonr(y_test, predicted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "print('Mean Absolute Error (MAE):', metrics.mean_absolute_error(y_test, predicted_test), file=open('Output/08CVAND_Metrics.txt', 'a'))\n",
    "print('Mean Squared Error (MSE):', metrics.mean_squared_error(y_test, predicted_test), file=open('Output/08CVAND_Metrics.txt', 'a'))\n",
    "print('Root Mean Squared Error (RMSE):', np.sqrt(metrics.mean_squared_error(y_test, predicted_test)), file=open('Output/08CVAND_Metrics.txt', 'a'))\n",
    "\n",
    "print(f'Out-of-bag R2 score estimate: {regr_rf.oob_score_:>5.3}', file=open('Output/08CVAND_Metrics.txt', 'a'))\n",
    "print(f'Test data R2 score: {test_score:>5.3}', file=open('Output/08CVAND_Metrics.txt', 'a'))\n",
    "print(f'Test data Spearman correlation: {spearman[0]:.3}', file=open('Output/08CVAND_Metrics.txt', 'a'))\n",
    "print(f'Test data Pearson correlation: {pearson[0]:.3}', file=open('Output/08CVAND_Metrics.txt', 'a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot results with Yellowbrick\n",
    "https://www.scikit-yb.org/en/latest/api/regressor/peplot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals plot\n",
    "f = plt.figure()\n",
    "visualizer = ResidualsPlot(regr_rf)\n",
    "visualizer.fit(X_train, y_train)\n",
    "visualizer.score(X_test, y_test)\n",
    "visualizer.show()\n",
    "f.savefig(\"Output/08CVAND_Residuals.png\", bbox_inches='tight', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction error plot\n",
    "f = plt.figure()\n",
    "visualizer = PredictionError(regr_rf)\n",
    "visualizer.fit(X_train, y_train)\n",
    "visualizer.score(X_test, y_test)\n",
    "visualizer.show()\n",
    "f.savefig(\"Output/08CVAND_PredActual.png\", bbox_inches='tight', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_CV2plot = pd.DataFrame(predicted_CV)\n",
    "df2plot = pd.concat([y_test, predicted_CV2plot], axis=1)\n",
    "df2plot.columns = ['index', 'actual', 'location', 'pred']\n",
    "df2plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#984ea3', '#e41a1c', '#ffff33', '#ff7f00', '#4daf4a', '#a65628', '#377eb8', '#f781bf', '#999999']\n",
    "#https://colorbrewer2.org/#type=qualitative&scheme=Set1&n=9\n",
    "\n",
    "plt.figure(figsize=(18,8))\n",
    "sns.set_palette(colors)\n",
    "sns.scatterplot(x='actual', y='pred', hue='location', s = 40, data=df2plot)\n",
    "plt.title(\"Actual vs predicted beta values\")\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "#plt.savefig('Output/08CV_PredActual_Color.png', bbox_inches='tight', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = regr_rf.feature_importances_         # get importance\n",
    "\n",
    "# summarize feature importance\n",
    "#for i,v in enumerate(importance):\n",
    "#\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "\n",
    "labels = list(X.columns.values)\n",
    "\n",
    "plt.figure(figsize=(18,8))\n",
    "imp = sns.barplot([x for x in range(len(importance))], importance)\n",
    "imp.set_xticklabels(labels,  rotation='vertical')\n",
    "plt.title(\"Feature importance\")\n",
    "#plt.savefig('Output/08CV_Features.png', bbox_inches='tight', dpi=600)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
